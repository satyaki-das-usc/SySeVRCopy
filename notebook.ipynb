{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import functools\n",
    "from argparse import ArgumentParser\n",
    "from functools import partial, wraps\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from multiprocessing import cpu_count, Manager, Pool, Queue\n",
    "\n",
    "from typing import cast, List, Dict, Set\n",
    "from tqdm.notebook import tqdm\n",
    "from os.path import isfile, isdir, join, exists, relpath\n",
    "from src.data_generator import read_csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoTokenizer\n",
    "from torch import nn, einsum\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from src.clip_mlm import *\n",
    "from src.clip_mlm import CLIP as CLIPMLM\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = \"Tushmit Chowdhury\"\n",
    "name2 = \"Satyaki Das\"\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 1\n",
    "torch.tensor(label).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create dummy data\n",
    "X = torch.randn(1000, 10)  # Input features\n",
    "y = torch.randint(0, 2, (1000,))  # Target labels\n",
    "\n",
    "print(\"Tushmit\")\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X, y)\n",
    "print(\"Tushmit\")\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "print(\"Tushmit\")\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "print(\"Tushmit\")\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for batch_X, batch_y in data_loader:\n",
    "    # Use the batch for training or evaluation\n",
    "    print(\"__debug__\")\n",
    "    print(\"Batch input shape:\", batch_X.shape)\n",
    "    print(\"Batch target shape:\", batch_y.shape)\n",
    "    break  # Only print the first batch for demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__debug__\n"
     ]
    }
   ],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, args, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.args = args\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        assert self.args.maskVV & self.args.maskVN != True\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        if self.args.maskVV:\n",
    "            return torch.mean((1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        elif self.args.maskVN:\n",
    "            return torch.mean((label) * torch.pow(euclidean_distance, 2))\n",
    "        else:\n",
    "            return torch.mean((label) * torch.pow(euclidean_distance, 2) +\n",
    "                              (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0),\n",
    "                                                      2))\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    return\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=42)\n",
    "\n",
    "parser.add_argument('--dataset', type=str, default='RE',\n",
    "                    choices=['RE', 'TD', 'IO'])\n",
    "\n",
    "# clip stage args\n",
    "parser.add_argument('--epoch_clip', type=int, default=100)\n",
    "parser.add_argument('--batch_size_clip', type=int, default=32)\n",
    "parser.add_argument('--lr_clip', type=float, default=1e-5)\n",
    "parser.add_argument('--save_epoch', type=int, default=1)\n",
    "parser.add_argument('--mlmloss', type=float, default=0.1)\n",
    "parser.add_argument('--maskVV', action='store_true')\n",
    "parser.add_argument('--maskVN', action='store_true')\n",
    "\n",
    "# classifier stage args\n",
    "parser.add_argument('--epoch_cla', type=int, default=20)\n",
    "parser.add_argument('--batch_size_cla', type=int, default=32)\n",
    "parser.add_argument('--lr_2', type=float, default=1e-5)\n",
    "parser.add_argument('--max_length', type=int, default=1024)\n",
    "parser.add_argument('--savepath', type=str, default='./Results/mlm')\n",
    "parser.add_argument('--resume', action='store_true')\n",
    "parser.add_argument('--resume_file', type=str, default=None)\n",
    "parser.add_argument('--train_clip', action='store_false')\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "set_seed(args.seed)\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c57f674502464784ee9001a54fa721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13525.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40f597b89f1450abade55b14de750d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11894.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open(\"data/SARD/function.json\", \"r\") as rfi:\n",
    "    dataset = json.load(rfi)\n",
    "\n",
    "with open(\"data/SARD/vv_vn_pairs.json\", \"r\") as rfi:\n",
    "    pairs = json.load(rfi)\n",
    "\n",
    "func1_list = []\n",
    "func2_list = []\n",
    "corr_label_list = []\n",
    "\n",
    "for pair in tqdm(pairs):\n",
    "    func1_batch = dataset[pair[\"idx\"]][\"raw_func\"]\n",
    "    for idx in pair[\"pairs\"][\"vv\"]:\n",
    "        func2_batch = dataset[idx][\"raw_func\"]\n",
    "        func1_list.append(func1_batch)\n",
    "        func2_list.append(func2_batch)\n",
    "        corr_label_list.append(1)\n",
    "    \n",
    "    for idx in pair[\"pairs\"][\"vn\"]:\n",
    "        func2_batch = dataset[idx][\"raw_func\"]\n",
    "        func1_list.append(func1_batch)\n",
    "        func2_list.append(func2_batch)\n",
    "        corr_label_list.append(0)\n",
    "    \n",
    "df = pd.DataFrame({\n",
    "    \"func1\": func1_list,\n",
    "    \"func2\": func2_list,\n",
    "    \"label\": corr_label_list\n",
    "})\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "func1_batches = []\n",
    "func2_batches = []\n",
    "corr_label_batches = []\n",
    "\n",
    "for i in range(0, len(corr_label_list), batch_size):\n",
    "    func1_batches.append(func1_list[i: i + batch_size])\n",
    "    func2_batches.append(func2_list[i: i + batch_size])\n",
    "    corr_label_batches.append(corr_label_list[i: i + batch_size])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "clip = CLIPMLM(\n",
    "            args=args,\n",
    "            dim_text=512,\n",
    "            num_text_tokens=50265,\n",
    "            text_seq_len=args.max_length,\n",
    "            text_heads=8\n",
    "        )\n",
    "optimizer = optim.AdamW(clip.parameters(), lr=args.lr_clip)\n",
    "c_loss = ContrastiveLoss(args=args)\n",
    "\n",
    "clip = torch.nn.DataParallel(clip, device_ids=[0, 1])\n",
    "model = clip.to(args.device)\n",
    "\n",
    "plot_data = []\n",
    "plot_label = []\n",
    "\n",
    "pbar = tqdm(zip(func1_batches, func2_batches, corr_label_batches), total=len(corr_label_batches))\n",
    "\n",
    "for func1_batch, func2_batch, label_batch in pbar:\n",
    "    ids_func1 = tokenizer(func1_batch, padding=True, truncation=True, return_tensors='pt',\n",
    "                                 max_length=args.max_length)['input_ids'].to(args.device)\n",
    "    ids_func2 = tokenizer(func2_batch, padding=True, truncation=True, return_tensors='pt',\n",
    "                                 max_length=args.max_length)['input_ids'].to(args.device)\n",
    "    corr_label = torch.tensor(label_batch).to(args.device)\n",
    "\n",
    "    CLS1, CLS2, ssl_loss = model(text1=ids_func1, text2=ids_func2, training_classifier=False)\n",
    "    loss = c_loss(CLS1, CLS2, corr_label)\n",
    "\n",
    "    plot_data.append(CLS1)\n",
    "    plot_label.append(label_batch)\n",
    "\n",
    "plot_data = torch.cat(plot_data, dim=0).to('cpu').detach().numpy()\n",
    "plot_label = torch.cat(plot_label).to('cpu').detach().numpy()\n",
    "plt.scatter(plot_data[:, 0], plot_data[:, 1], c=plot_label)\n",
    "plt.savefig(\"data/SARD/corr_plot_fig.png\")\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/SARD/function.json\", \"r\") as rfi:\n",
    "    dataset = json.load(rfi)\n",
    "\n",
    "vv_vn_pairs = dict()\n",
    "\n",
    "for idx, entry in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    if entry[\"target\"] == 0:\n",
    "        continue\n",
    "    file_name = entry[\"file_name\"]\n",
    "    same_file_samples = [(i, data) for i, data in enumerate(dataset) if data[\"file_name\"] == file_name]\n",
    "    for i, data in same_file_samples:\n",
    "        if idx == i:\n",
    "            continue\n",
    "        if str(idx) not in vv_vn_pairs:\n",
    "            vv_vn_pairs[str(idx)] = {\n",
    "                \"vv\": [],\n",
    "                \"vn\": []\n",
    "            }\n",
    "        if data[\"target\"] == 1:\n",
    "            vv_vn_pairs[str(idx)][\"vv\"].append(i)\n",
    "        else:\n",
    "            vv_vn_pairs[str(idx)][\"vn\"].append(i)\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/SARD/vv_vn_pairs.json\", \"w\") as wfi:\n",
    "    json.dump(vv_vn_pairs, wfi, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/SARD/function.json\", \"r\") as rfi:\n",
    "    dataset = json.load(rfi)\n",
    "\n",
    "dataset_pos = [entry for entry in dataset if entry[\"target\"] == 1]\n",
    "\n",
    "file_name = dataset_pos[0][\"file_name\"]\n",
    "same_file_samples = [entry for entry in dataset if entry[\"file_name\"] == file_name]\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(csv_file_path: str) -> List:\n",
    "    \"\"\"\n",
    "    read csv file\n",
    "    \"\"\"\n",
    "    assert exists(csv_file_path), f\"no {csv_file_path}\"\n",
    "    data = []\n",
    "    with open(csv_file_path) as fp:\n",
    "        header = fp.readline()\n",
    "        header = header.strip()\n",
    "        h_parts = [hp.strip() for hp in header.split('\\t')]\n",
    "        for line in fp:\n",
    "            line = line.strip()\n",
    "            instance = {}\n",
    "            lparts = line.split('\\t')\n",
    "            for i, hp in enumerate(h_parts):\n",
    "                if i < len(lparts):\n",
    "                    content = lparts[i].strip()\n",
    "                else:\n",
    "                    content = ''\n",
    "                instance[hp] = content\n",
    "            data.append(instance)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CPU = cpu_count()\n",
    "ground_truth_set = dict()\n",
    "prefix = \"\"\n",
    "dataset_root = \"data/SARD\"\n",
    "source_root_path = join(dataset_root, \"source-code\")\n",
    "csv_path = join(dataset_root, \"csv\")\n",
    "ground_truth_path = join(dataset_root, \"ground_truth.json\")\n",
    "\n",
    "with open(ground_truth_path, \"r\") as rfi:\n",
    "    ground_truth = json.load(rfi)\n",
    "\n",
    "cpp_paths = []\n",
    "cpp_paths_filepath = join(dataset_root, \"cpp_paths.json\")\n",
    "with open(cpp_paths_filepath, \"r\") as rfi:\n",
    "    cpp_paths = json.load(rfi)\n",
    "\n",
    "cpp_path = cpp_paths[44705-2]\n",
    "file_vul_lines = set()\n",
    "if cpp_path in ground_truth:\n",
    "    file_vul_lines = set(ground_truth[cpp_path])\n",
    "SRC_PATH = join(source_root_path, cpp_path)\n",
    "with open(SRC_PATH, \"r\") as rfi:\n",
    "    src_lines = rfi.readlines()\n",
    "nodes_dir = join(csv_path, cpp_path)\n",
    "joern_nodes = read_csv(join(nodes_dir, \"nodes.csv\"))\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CPU = cpu_count()\n",
    "ground_truth_set = dict()\n",
    "prefix = \"\"\n",
    "dataset_root = \"data/SARD\"\n",
    "source_root_path = join(dataset_root, \"source-code\")\n",
    "csv_path = join(dataset_root, \"csv\")\n",
    "ground_truth_path = join(dataset_root, \"ground_truth.json\")\n",
    "\n",
    "all_cpp_paths = []\n",
    "\n",
    "for root, dirs, files in os.walk(source_root_path, topdown=True):\n",
    "    cpp_filenames = [filename for filename in files if filename.endswith(\".cpp\") or filename.endswith(\".c\") or filename.endswith(\".h\") or filename.endswith(\".hpp\") or filename.endswith(\".cc\") or filename.endswith(\".hh\")]\n",
    "    if len(cpp_filenames) == 0:\n",
    "        continue\n",
    "    all_cpp_paths += [join(relpath(root, source_root_path), filename) for filename in cpp_filenames]\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = join(dataset_root, \"cpp_paths.json\")\n",
    "\n",
    "with open(filepath, \"w\") as wfi:\n",
    "    json.dump(all_cpp_paths, wfi, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCodeIDtoPathDict(testcases: List) -> Dict[str, Dict[str, Set[int]]]:\n",
    "    '''build code testcaseid to path map\n",
    "\n",
    "    use the manifest.xml. build {testcaseid:{filePath:set(vul lines)}}\n",
    "    filePath use relevant path, e.g., CWE119/cve/source-code/project_commit/...\n",
    "    :param testcases:\n",
    "    :return: {testcaseid:{filePath:set(vul lines)}}\n",
    "    '''\n",
    "    codeIDtoPath: Dict[str, Dict[str, Set[int]]] = {}\n",
    "    for testcase in testcases:\n",
    "        files = testcase.findall(\"file\")\n",
    "        testcaseid = testcase.attrib[\"id\"]\n",
    "        codeIDtoPath[testcaseid] = dict()\n",
    "\n",
    "        for file in files:\n",
    "            path = file.attrib[\"path\"]\n",
    "            flaws = file.findall(\"flaw\")\n",
    "            mixeds = file.findall(\"mixed\")\n",
    "            fix = file.findall(\"fix\")\n",
    "            # print(mixeds)\n",
    "            VulLine = set()\n",
    "            if (flaws != [] or mixeds != [] or fix != []):\n",
    "                # targetFilePath = path\n",
    "                if (flaws != []):\n",
    "                    for flaw in flaws:\n",
    "                        VulLine.add(int(flaw.attrib[\"line\"]))\n",
    "                if (mixeds != []):\n",
    "                    for mixed in mixeds:\n",
    "                        VulLine.add(int(mixed.attrib[\"line\"]))\n",
    "\n",
    "            codeIDtoPath[testcaseid][path] = VulLine\n",
    "\n",
    "    return codeIDtoPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_path = \"data/SARD/SARD_testcaseinfo.xml\"\n",
    "tree = ET.ElementTree(file=xml_path)\n",
    "testcases = tree.findall(\"testcase\")\n",
    "pathToCodeID = getCodeIDtoPathDict(testcases)\n",
    "\n",
    "ground_truth_set = dict()\n",
    "for test_ID, file_vul_lines in pathToCodeID.items():\n",
    "    for cpp_path, vul_lines in file_vul_lines.items():\n",
    "        actual_vul_lines = [line for line in vul_lines if line > 0]\n",
    "        if len(actual_vul_lines) == 0:\n",
    "            continue\n",
    "        if cpp_path not in all_cpp_paths:\n",
    "            continue\n",
    "        if cpp_path not in ground_truth_set:\n",
    "            ground_truth_set[cpp_path] = set()\n",
    "        ground_truth_set[cpp_path] = ground_truth_set[cpp_path].union(actual_vul_lines)\n",
    "\n",
    "ground_truth = {cpp_path: list(vul_lines) for cpp_path, vul_lines in ground_truth_set.items()}\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ground_truth_path, \"w\") as wfi:\n",
    "    json.dump(ground_truth, wfi, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcases_filtered = [testcase for testcase in testcases if testcase.attrib[\"id\"] == \"4\"]\n",
    "\n",
    "testcase = testcases_filtered[0]\n",
    "\n",
    "files = testcase.findall(\"file\")\n",
    "testcaseid = testcase.attrib[\"id\"]\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = testcases[100]\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_ground_truth_path = \"data/SARD/SARD_testcaseinfo.txt\"\n",
    "\n",
    "with open(txt_ground_truth_path, \"r\") as rfi:\n",
    "    all_lines = [line.strip().replace(\"../SySeVRCopy/data/SARD/source-code/\", \"\") for line in rfi.readlines()]\n",
    "\n",
    "ground_truth_set = dict()\n",
    "\n",
    "for line in  all_lines:\n",
    "    cpp_path, line_num = [part.strip() for part in line.split()]\n",
    "    line_num = int(line_num)\n",
    "    if line_num == 0:\n",
    "        continue\n",
    "    if cpp_path not in ground_truth_set:\n",
    "        ground_truth_set[cpp_path] = set()\n",
    "    ground_truth_set[cpp_path].add(line_num)\n",
    "\n",
    "ground_truth_set = {cpp_path: list(vul_lines) for cpp_path, vul_lines in ground_truth_set.items()}\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from os.path import isdir, isfile, join, exists, relpath\n",
    "from tqdm.notebook import tqdm\n",
    "from slice_labeling.program_analysis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_filepath = \"data/SARD/API function call.txt\"\n",
    "\n",
    "with open(slice_filepath, \"r\") as rfi:\n",
    "    slicelists = rfi.read().split(\"------------------------------\")\n",
    "\n",
    "if slicelists[0] == '':\n",
    "    del slicelists[0]\n",
    "if slicelists[-1] == '' or slicelists[-1] == '\\n' or slicelists[-1] == '\\r\\n':\n",
    "    del slicelists[-1]\n",
    "\n",
    "curr_slice = slicelists[0]\n",
    "slice_lines = [line.strip() for line in curr_slice.splitlines()]\n",
    "idx, src_filepath, func_name, line_num = [part.strip() for part in  slice_lines[0].split()]\n",
    "\n",
    "test_id, filename = [part.strip() for part in src_filepath.split(\"/\")]\n",
    "\n",
    "csv_root = \"data/SARD/csv\"\n",
    "src_root = \"data/SARD/source-code\"\n",
    "\n",
    "cpp_filepath = \"\"\n",
    "for root, dirs, files in os.walk(src_root, topdown=True):\n",
    "    if len(files) == 0:\n",
    "        continue\n",
    "    dir_test_id = relpath(root, src_root).replace(\"/\", \"\").lstrip(\"0\")\n",
    "    if dir_test_id != test_id:\n",
    "        continue\n",
    "    cpp_filepath = join(relpath(root, src_root), filename)\n",
    "    break\n",
    "\n",
    "nodes_dir = join(csv_root, cpp_filepath)\n",
    "\n",
    "nodes_path = join(nodes_dir, \"nodes.csv\")\n",
    "edges_path = join(nodes_dir, \"edges.csv\")\n",
    "nodes = read_csv(nodes_path)\n",
    "edges = read_csv(edges_path)\n",
    "\n",
    "slice_funcname = slice_lines[1].split()[-1].split('(')[0]\n",
    "\n",
    "func_nodes = [(idx, entry) for idx, entry in enumerate(nodes) if entry[\"type\"] == \"Function\"]\n",
    "\n",
    "slice_func_idx = [(idx) for (idx, entry) in enumerate(func_nodes) if entry[1][\"code\"].strip() == slice_funcname][0]\n",
    "\n",
    "start_idx = func_nodes[slice_func_idx][0]\n",
    "end_idx = func_nodes[slice_func_idx + 1][0]\n",
    "\n",
    "for line in slice_lines[1:-1]:\n",
    "    line_content = line.strip().replace(\" \", \"\")\n",
    "    for node in nodes[start_idx:end_idx]:\n",
    "        node_content = node[\"code\"].strip().replace(\" \", \"\")\n",
    "        if line_content == node_content:\n",
    "            loc = node[\"location\"].split(\":\")[0].strip()\n",
    "            print(f\"{line} {loc}\")\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_file = \"data/SARD/API function call.txt\"\n",
    "\n",
    "with open(slice_file, \"r\") as rfi:\n",
    "    slicelists = rfi.read().split(\"------------------------------\")\n",
    "\n",
    "if slicelists[0] == '':\n",
    "    del slicelists[0]\n",
    "if slicelists[-1] == '' or slicelists[-1] == '\\n' or slicelists[-1] == '\\r\\n':\n",
    "    del slicelists[-1]\n",
    "\n",
    "for slicelist in tqdm(slicelists):\n",
    "    slice_corpus = []\n",
    "    focus_index = 0\n",
    "    flag_focus = 0\n",
    "\n",
    "    sentences = slicelist.split('\\n')\n",
    "\n",
    "    if sentences[0] == '\\r' or sentences[0] == '':\n",
    "        del sentences[0]\n",
    "    if sentences == []:\n",
    "        continue\n",
    "    if sentences[-1] == '':\n",
    "        del sentences[-1]\n",
    "    if sentences[-1] == '\\r':\n",
    "        del sentences[-1]\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"65156/CWE121_Stack_Based_Buffer_Overflow__CWE805_wchar_t_alloca_ncat_05.c\"\n",
    "csv_root = \"data/SARD/csv\"\n",
    "src_root = \"data/SARD/source-code\"\n",
    "\n",
    "test_id = file_path.split(\"/\")[0]\n",
    "filename = file_path.split(\"/\")[-1]\n",
    "src_filepath = \"\"\n",
    "for root, dirs, files in os.walk(src_root, topdown=True):\n",
    "    if len(files) == 0:\n",
    "        continue\n",
    "    dir_test_id = relpath(root, src_root).replace(\"/\", \"\").lstrip(\"0\")\n",
    "    if dir_test_id != test_id:\n",
    "        continue\n",
    "    src_filepath = join(relpath(root, src_root), filename)\n",
    "    break\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"slice_labeling/sample.txt\", \"r\") as rfi:\n",
    "    sample_slice = [line.strip() for line in rfi.readlines()]\n",
    "\n",
    "nodes_dir = join(csv_root, src_filepath)\n",
    "\n",
    "nodes_path = join(nodes_dir, \"nodes.csv\")\n",
    "edges_path = join(nodes_dir, \"edges.csv\")\n",
    "nodes = read_csv(nodes_path)\n",
    "edges = read_csv(edges_path)\n",
    "\n",
    "# PDG = build_PDG_no_post_dom(nodes_dir, src_filepath)\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from joern.all import JoernSteps\n",
    "\n",
    "from os.path import isdir, exists, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SARD/callee_CFGNode_map.pkl\", \"rb\") as rbfi:\n",
    "    data = pickle.load(rbfi)\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = \"SARD/dict_call2cfgNodeID_funcID\"\n",
    "\n",
    "filename = \"dict.pkl\"\n",
    "\n",
    "for test_id in os.listdir(dirpath):\n",
    "    test_id_dirpath = join(dirpath, test_id)\n",
    "    with open(join(test_id_dirpath, filename), \"rb\") as rbfi:\n",
    "        data = pickle.load(rbfi)\n",
    "    print(\"__debug__\")\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo pip install git+https://github.com/fabsx00/python-joern.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "~/joern/joern-parse /home/satyaki/luka/SySeVRCopy/data/SARD/source-code\n",
    "\n",
    "/home/satyaki/luka/SySeVRCopy/joern/bin/joern\n",
    "\n",
    "joern/bin/joern/joern-cli/joern-parse /home/satyaki/luka/SySeVRCopy/data/SARD/source-code\n",
    "joern/bin/joern/joern-cli/joern-parse /home/satyaki/luka/SySeVRCopy/data/SARD/source-code/000/001/002\n",
    "joern/bin/joern/joern-cli/joern-export --repr=all --format=neo4j\n",
    "\n",
    "export JAVA_HOME=\"/usr/lib/jvm/jdk-17\"\n",
    "export PATH=$JAVA_HOME/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sard_dir = \"data/SARD/SARD\"\n",
    "\n",
    "total_files = 0\n",
    "for root, dirs, files in os.walk(sard_dir):\n",
    "    src_files = [file for file in files if file.endswith(\".c\") or file.endswith(\".cpp\") or file.endswith(\".h\")]\n",
    "    total_files += len(src_files)\n",
    "\n",
    "print(\"__debug__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHONPATH=\".\" python SySeVR_docker/docker_build/home/SySeVR/softdir/python-joern-0.3.1/setup.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
